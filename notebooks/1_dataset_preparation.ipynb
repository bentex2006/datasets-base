{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d07cee0",
   "metadata": {},
   "source": [
    "# ðŸš€ LLM Dataset Preparation and Fine-tuning\n",
    "\n",
    "This notebook works across different platforms:\n",
    "- Google Colab\n",
    "- Kaggle\n",
    "- Local Jupyter\n",
    "- Cloud clusters\n",
    "\n",
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05451b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# Detect runtime environment\n",
    "def get_environment():\n",
    "    try:\n",
    "        import google.colab\n",
    "        return 'colab'\n",
    "    except:\n",
    "        try:\n",
    "            import kaggle\n",
    "            return 'kaggle'\n",
    "        except:\n",
    "            return 'local'\n",
    "\n",
    "ENV = get_environment()\n",
    "print(f\"Running in {ENV} environment\")\n",
    "\n",
    "# Install required packages\n",
    "!pip install -q transformers datasets torch accelerate bitsandbytes tqdm pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae9ebde",
   "metadata": {},
   "source": [
    "## Clone Repository and Setup Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b957f821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/bentex2006/datasets-base.git\n",
    "!cd datasets-base\n",
    "\n",
    "# Setup paths based on environment\n",
    "if ENV == 'colab':\n",
    "    BASE_PATH = '/content/datasets-base'\n",
    "elif ENV == 'kaggle':\n",
    "    BASE_PATH = '/kaggle/working/datasets-base'\n",
    "else:\n",
    "    BASE_PATH = './datasets-base'\n",
    "\n",
    "DATA_PATH = os.path.join(BASE_PATH, 'data')\n",
    "RAW_DATA_PATH = os.path.join(DATA_PATH, 'raw')\n",
    "PROCESSED_DATA_PATH = os.path.join(DATA_PATH, 'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2976c0",
   "metadata": {},
   "source": [
    "## Load and Process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2cde57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_jsonl(file_path: str) -> List[Dict]:\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Load sample datasets\n",
    "instruction_data = load_jsonl(os.path.join(RAW_DATA_PATH, 'sample_instruction_dataset.jsonl'))\n",
    "conversation_data = load_jsonl(os.path.join(RAW_DATA_PATH, 'sample_conversation_dataset.jsonl'))\n",
    "completion_data = load_jsonl(os.path.join(RAW_DATA_PATH, 'sample_completion_dataset.jsonl'))\n",
    "hinglish_data = load_jsonl(os.path.join(RAW_DATA_PATH, 'savage_ai_hinglish.jsonl'))\n",
    "\n",
    "print(f\"Loaded datasets:\")\n",
    "print(f\"- Instructions: {len(instruction_data)} examples\")\n",
    "print(f\"- Conversations: {len(conversation_data)} examples\")\n",
    "print(f\"- Completions: {len(completion_data)} examples\")\n",
    "print(f\"- Hinglish: {len(hinglish_data)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eeb76f",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edd4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_mistral(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Format data for Mistral fine-tuning\"\"\"\n",
    "    return [{\n",
    "        \"instruction\": item.get(\"instruction\", \"\"),\n",
    "        \"input\": item.get(\"input\", \"\"),\n",
    "        \"output\": item.get(\"output\", \"\")\n",
    "    } for item in data]\n",
    "\n",
    "def format_for_pi(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Format data for Pi fine-tuning\"\"\"\n",
    "    return [{\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"human\", \"content\": item.get(\"input\", \"\")},\n",
    "            {\"role\": \"assistant\", \"content\": item.get(\"output\", \"\")}\n",
    "        ]\n",
    "    } for item in data]\n",
    "\n",
    "def format_for_gemini(data: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Format data for Gemini fine-tuning\"\"\"\n",
    "    return [{\n",
    "        \"prompt\": item.get(\"input\", \"\"),\n",
    "        \"completion\": item.get(\"output\", \"\"),\n",
    "        \"context\": item.get(\"instruction\", \"\")\n",
    "    } for item in data]\n",
    "\n",
    "def save_jsonl(data: List[Dict], file_path: str) -> None:\n",
    "    \"\"\"Save data in JSONL format\"\"\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da8b81",
   "metadata": {},
   "source": [
    "## Process Data for Different Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f303d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Hinglish data for different models\n",
    "mistral_data = format_for_mistral(hinglish_data)\n",
    "pi_data = format_for_pi(hinglish_data)\n",
    "gemini_data = format_for_gemini(hinglish_data)\n",
    "\n",
    "# Save processed data\n",
    "save_jsonl(mistral_data, os.path.join(PROCESSED_DATA_PATH, 'hinglish_mistral.jsonl'))\n",
    "save_jsonl(pi_data, os.path.join(PROCESSED_DATA_PATH, 'hinglish_pi.jsonl'))\n",
    "save_jsonl(gemini_data, os.path.join(PROCESSED_DATA_PATH, 'hinglish_gemini.jsonl'))\n",
    "\n",
    "print(\"Data processed and saved for all models!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
