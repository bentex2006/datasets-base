{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de40399",
   "metadata": {},
   "source": [
    "# QLoRA Fine-tuning for Multi-turn Dialogue Models\n",
    "\n",
    "This notebook demonstrates how to fine-tune language models for multi-turn dialogue using QLoRA (Quantized Low-Rank Adaptation). We'll use the dialogue format dataset we prepared earlier.\n",
    "\n",
    "## Setup and Requirements\n",
    "- PEFT for efficient fine-tuning\n",
    "- Transformers for model handling\n",
    "- bitsandbytes for 4-bit quantization\n",
    "- Accelerate for distributed training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4ae2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers==4.36.2 bitsandbytes==0.41.1 accelerate==0.25.0 peft==0.7.1 torch==2.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e0993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375479f8",
   "metadata": {},
   "source": [
    "## Load Configuration\n",
    "\n",
    "We'll load the QLoRA configuration from our config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a48ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config/model_configs/mistral_qlora_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Extract model configuration\n",
    "model_name = config['model_name']\n",
    "lora_r = config['lora_r']\n",
    "lora_alpha = config['lora_alpha']\n",
    "lora_dropout = config['lora_dropout']\n",
    "print(f\"Using model: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390ddee",
   "metadata": {},
   "source": [
    "## Download and Load the Base Model\n",
    "\n",
    "We'll download and load the base model with 4-bit quantization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674dbd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Download and load the model\n",
    "print(f\"Downloading {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Download and load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(\"Model and tokenizer loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4a927",
   "metadata": {},
   "source": [
    "## Configure LoRA\n",
    "\n",
    "Set up the LoRA configuration for efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c19f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "# Get PEFT model\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"LoRA configuration applied successfully!\")\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e0a2f5",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset\n",
    "\n",
    "We'll load our preprocessed dialogue dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e53d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DialogueDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        self.conversations = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                self.conversations.append(json.loads(line))\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        conversation = self.conversations[idx]\n",
    "        # Format the dialogue turns\n",
    "        formatted_text = \"\"\n",
    "        for turn in conversation['turns']:\n",
    "            formatted_text += f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\\n\"\n",
    "        \n",
    "        # Tokenize\n",
    "        encodings = self.tokenizer(formatted_text, \n",
    "                                  truncation=True, \n",
    "                                  max_length=2048,\n",
    "                                  padding=\"max_length\",\n",
    "                                  return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = DialogueDataset('../data/processed/dialogue_format.jsonl', tokenizer)\n",
    "print(f\"Loaded {len(train_dataset)} dialogue examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c2c47",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "Configure the training parameters and prepare the trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19869e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/dialogue_qlora\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=3,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=lambda data: {\n",
    "        'input_ids': torch.stack([f['input_ids'] for f in data]),\n",
    "        'attention_mask': torch.stack([f['attention_mask'] for f in data]),\n",
    "        'labels': torch.stack([f['input_ids'] for f in data])\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ee40d",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Begin the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9af7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(\"../models/dialogue_qlora/final\")\n",
    "print(\"Training completed and model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c575ac",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "Let's test the fine-tuned model with a sample dialogue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c3e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=200):\n",
    "    inputs = tokenizer(f\"User: {prompt}\\nAssistant:\", return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test with a sample prompt\n",
    "test_prompt = \"What's your favorite book and why?\"\n",
    "response = generate_response(test_prompt)\n",
    "print(f\"User: {test_prompt}\")\n",
    "print(f\"Assistant: {response}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
