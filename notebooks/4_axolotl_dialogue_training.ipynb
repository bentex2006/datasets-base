{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70547114",
   "metadata": {},
   "source": [
    "# Dialogue Fine-tuning with Axolotl\n",
    "\n",
    "This notebook demonstrates how to use Axolotl for dialogue model fine-tuning. Axolotl is a powerful tool that simplifies the process of fine-tuning language models, especially for dialogue tasks, by handling:\n",
    "\n",
    "- Proper dialogue formatting\n",
    "- Context window management\n",
    "- Multi-turn conversation handling\n",
    "- Efficient training configurations\n",
    "- QLoRA integration\n",
    "\n",
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1a1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Axolotl and dependencies\n",
    "!pip install -q git+https://github.com/OpenAccess-AI-Collective/axolotl\n",
    "!pip install -q accelerate bitsandbytes wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e8210c",
   "metadata": {},
   "source": [
    "## Create Axolotl Configuration\n",
    "\n",
    "Axolotl uses YAML configuration files. Let's create one for our dialogue fine-tuning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6bb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../config/model_configs/axolotl_dialogue_config.yml\n",
    "base_model: mistralai/Mistral-7B-v0.1\n",
    "model_config:\n",
    "  trust_remote_code: true\n",
    "  use_flash_attention_2: true\n",
    "\n",
    "datasets:\n",
    "  - path: ../data/processed/dialogue_format.jsonl\n",
    "    type: jsonl\n",
    "    format: chatml\n",
    "    conversation:\n",
    "      turns_key: turns\n",
    "      user_key: user\n",
    "      assistant_key: assistant\n",
    "\n",
    "sequence_len: 2048\n",
    "sample_packing: true\n",
    "pad_to_sequence_len: true\n",
    "\n",
    "adapter: qlora\n",
    "lora_model_dir: ../models/axolotl_dialogue\n",
    "\n",
    "lora_r: 32\n",
    "lora_alpha: 16\n",
    "lora_dropout: 0.05\n",
    "lora_target_modules:\n",
    "  - q_proj\n",
    "  - v_proj\n",
    "  - k_proj\n",
    "  - o_proj\n",
    "  - gate_proj\n",
    "  - up_proj\n",
    "  - down_proj\n",
    "\n",
    "load_in_4bit: true\n",
    "bf16: true\n",
    "flash_attention: true\n",
    "\n",
    "micro_batch_size: 4\n",
    "gradient_accumulation_steps: 4\n",
    "num_epochs: 3\n",
    "learning_rate: 2e-4\n",
    "warmup_steps: 100\n",
    "save_steps: 100\n",
    "logging_steps: 10\n",
    "weight_decay: 0.001\n",
    "\n",
    "eval_steps: 50\n",
    "save_total_limit: 3\n",
    "optimizer: adamw_torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cdc297",
   "metadata": {},
   "source": [
    "## Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262dd626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from axolotl.utils.config import load_config\n",
    "from axolotl.utils.dict import DictDefault"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b5ab97",
   "metadata": {},
   "source": [
    "## Load and Verify Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b703d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../config/model_configs/axolotl_dialogue_config.yml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    \n",
    "cfg = DictDefault(cfg)\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Base model: {cfg.base_model}\")\n",
    "print(f\"Dataset path: {cfg.datasets[0]['path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08060051",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "Axolotl provides a CLI for training, but we can also run it programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c7cb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Axolotl training\n",
    "!accelerate launch -m axolotl.cli.train ../config/model_configs/axolotl_dialogue_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16d0b4",
   "metadata": {},
   "source": [
    "## Load and Test the Fine-tuned Model\n",
    "\n",
    "After training, we can load and test our fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c468e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "def load_model(base_model_path, adapter_path):\n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_path,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        load_in_4bit=True\n",
    "    )\n",
    "    \n",
    "    # Load adapter\n",
    "    model = PeftModel.from_pretrained(model, adapter_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "adapter_path = \"../models/axolotl_dialogue\"\n",
    "model, tokenizer = load_model(model_path, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a63094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=200):\n",
    "    # Format prompt for chat\n",
    "    chat_prompt = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(chat_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_prompt = \"What's your favorite book and why?\"\n",
    "response = generate_response(test_prompt)\n",
    "print(f\"User: {test_prompt}\")\n",
    "print(f\"Assistant: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0940ad",
   "metadata": {},
   "source": [
    "## Multi-turn Conversation Test\n",
    "\n",
    "Let's test the model with a multi-turn conversation to see how it handles context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445295c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversation(conversation_history=\"\", user_input=\"\"):\n",
    "    if conversation_history:\n",
    "        prompt = conversation_history + f\"\\n<|im_start|>user\\n{user_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    else:\n",
    "        prompt = f\"<|im_start|>user\\n{user_input}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=2048,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test multi-turn conversation\n",
    "conversation = \"\"\n",
    "turns = [\n",
    "    \"Hi! Can you help me learn about machine learning?\",\n",
    "    \"What should I learn first: supervised or unsupervised learning?\",\n",
    "    \"Can you give me a simple example of supervised learning?\"\n",
    "]\n",
    "\n",
    "for turn in turns:\n",
    "    print(f\"\\nUser: {turn}\")\n",
    "    conversation = chat_conversation(conversation, turn)\n",
    "    print(f\"Assistant: {conversation}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
